{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02_naive_bayes_Mandy He.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4p_DvtT7sOIr",
        "outputId": "08c254e6-65f6-4fcd-e1fb-c8ab0022f3c3"
      },
      "source": [
        "#import needed packages into colab\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "#Mounted the google drive to make files readable.\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_"
      },
      "source": [
        "# Data cleaning\n",
        "# Remove the non required words, expressions and symbols from text.\n",
        "\n",
        "def make_Dictionary(root_dir):                # define a new function\n",
        "  all_words_list = []                         # create a empty list where we can put most common words in it.\n",
        "  allemails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]   # create a list called allemails\n",
        "                                              # concatenates various path components with exactly one directory separator into a string\n",
        "                                              # and also retrieves the list with all the files in it\n",
        "  for mail in allemails:                      # read single item in 'allemails' list; define mail inside the allemails\n",
        "    with open(mail) as m:                     # after find mail, we will open each of the mail, and give it a name 'm'\n",
        "      for line in m:                          # loop single item in single file 'm'\n",
        "        words = line.split()                  # split each line (string) and convert them into a list\n",
        "        all_words_list += words               # add all words into the empty dictionary list\n",
        "  dictionary_lookup = Counter(all_words_list) # create a new container called dictionary_lookup, which contains all of the words we found for the dictionary, and will list how many times equivalent values are added\n",
        "  list_to_remove = list(dictionary_lookup)    # concert the 'dictionary_lookup' into a list, so that we can search for common words later.\n",
        "                                         \n",
        "\n",
        "  for singleitem in list_to_remove:                        #iterate the list\n",
        "    if singleitem.isalpha() == False:           \n",
        "      del dictionary_lookup[singleitem]                    #remove all the non-alpha numeric characters\n",
        "    elif len(singleitem) == 1:                  \n",
        "      del dictionary_lookup[singleitem]                    #remove any single character alpha-numeric characters \n",
        "  dictionary_lookup = dictionary_lookup.most_common(3000)  # only keep the most common 3000 words in the dictionary_lookup list  \n",
        "  return dictionary_lookup                                 # return the dictionary_lookup\n",
        "\n",
        "  # In concusion, after this part of data collection and data cleaning, we can get a 3000 most common words from the train dataset.\n",
        "  # Spam emails have already becoming a huge problem on the internet, with Machine Learning, we can easily create a dictionary \n",
        "  # which contain most of the common words when we consider a spam email."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "source": [
        "# extract features\n",
        "\n",
        "def extract_features(mail_dir):                               #Define a new function to extract the data\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)] # create a list called files\n",
        "                                                              # concatenates various path components with exactly one directory separator into a string\n",
        "                                                              # and also retrieves the list with all the files in it\n",
        "  features_matrix = np.zeros((len(files),3000))               # Also use np.zero function to return a new array with 3000 columns and the same number of email files as rows with zeros.(template empty)\n",
        "  train_labels = np.zeros(len(files))                         # return another array contains the number of email files as row numbers.\n",
        "  count = 1;                                                  # start of counter set to 1\n",
        "  docID = 0;                                                  # start of docID set to 0\n",
        "  for file in files:                                          # loop each file from 'files' list\n",
        "    with open(file) as f:                                     # open each single file as f\n",
        "      for itemcount, item in enumerate(f):                    # itemcount is the count of items, item is the item itself. Use enumerate method adds counter to an iterable and returns it, the returned object is a enumerate object\n",
        "        if itemcount == 2:                                    # if the count is 2\n",
        "          words = item.split()                                # we will split the line in to a list called words\n",
        "          for word in words:                                  # read each single item in list 'words'\n",
        "            wordID = 0                                        # set wordID to 0\n",
        "            for itemcount, item2 in enumerate(dictionary_lookup):   \n",
        "                                                              # loop dictionary_lookup list. itemcount is the count of items, and item2 is the item in dictionary_lookup\n",
        "              if item2[0] == word:                            # if the first index/item in the list equal to word (in words)\n",
        "                wordID = itemcount                            # then we assign itemcount to wordID \n",
        "                features_matrix[docID,wordID] = words.count(word)    #count how many unique word in words   [1,1,2,3]   2：1， 1：2， 1：3\n",
        "      train_labels[docID] = 0;                                       # set train_labels as 0\n",
        "      filepathTokens = file.split('/')                               # split each character backslash\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]              # since python count from 0, we have to minus one as it is last word\n",
        "      if lastToken.startswith(\"spmsg\"):                       # according to the case instruction, when an title of an email begin with 'spmsg' is means file is of spam emails.\n",
        "                                                              # The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. \n",
        "                                                              # Based on this the function also creates the Labelled Data Column\n",
        "        train_labels[docID] = 1;                              # set train_label as 1\n",
        "        count = count + 1                                     # the number of count will be added one by each loop\n",
        "      docID = docID + 1                                       # the number of count of docID will be added one by each loop\n",
        "  return features_matrix, train_labels                        #turn the results\n",
        "\n",
        "  #In summary, after this part of coding, we will get a new function\n",
        "  #This function that can help us to extract the training dataset and the testing dataset \n",
        "  #in order to return the Feature Dataset and the Label column."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp"
      },
      "source": [
        "#import Train & Test data\n",
        "\n",
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML Algorithms/Naive_Bayes/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML Algorithms/Naive_Bayes/test-mails'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "bf2f8097-8c84-4648-b54a-14b017e61d01"
      },
      "source": [
        "#Gaussian model is apply for the classification and it assumes that features follow a normal distribution.\n",
        "#call two functions\n",
        "\n",
        "dictionary_lookup = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)           \n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)  \n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)                           #use train data to train the model\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)       #use test data to perdict\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))        \n",
        "\n",
        "# Modle scores the test data by running the trained model with the test data.\n",
        "# Accurancy score is the percentage of correct perdiction, which is around 0.96 after we run the model. Also means this model is accurate for perdiction."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ]
}