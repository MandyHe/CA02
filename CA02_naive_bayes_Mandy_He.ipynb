{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02_naive_bayes_Mandy He.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCZYXwtCsL_y"
      },
      "source": [
        "This is a eMail Spam Classifers that uses Naive Bayes supervised machine learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4p_DvtT7sOIr",
        "outputId": "c7fbc881-cae7-4085-f47d-2675873601da"
      },
      "source": [
        "#import needed packages into colab\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from google.colab import drive\n",
        "\n",
        "#Mounted the google drive to make files readable.\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXknSIrLvzfQ"
      },
      "source": [
        "This function builds a Dictionary of most common 3000 words from all the email content. First it adds all words and symbols in the dictionary. Then it removes all non-alpha-numeric characters and any single character alpha-numeric characters. After this is complete it shrinks the Dictionary by keeping only most common 3000 words in the dictionary. It returns the Dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jjKF0nIMwz8_"
      },
      "source": [
        "# Data cleaning\n",
        "# Remove the non required words, expressions and symbols from text.\n",
        "\n",
        "def make_Dictionary(root_dir):                # define a new function\n",
        "  all_words_list = []                         # create a empty list dictionary where put most common words in it.\n",
        "  allemails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]   \n",
        "                                              # concatenates various path components with exactly one directory separator (‘/’), and also open the list containing the names of the entries in the directory given by path\n",
        "  for mail in allemails:                      # read single item in 'allemails' list; define mail inside the allemails\n",
        "    with open(mail) as m:                     # after find mail, we will open each of the mail, and give it a name 'm'\n",
        "      for line in m:                          # read single line in single file 'm'\n",
        "        words = line.split()                  # split each line (string) into a list\n",
        "        all_words_list += words               # add new words(list) into the all words empty dictionary list\n",
        "  dictionary_lookup = Counter(all_words_list) # create a new container called dictionary_lookup, which contains all of the words we found for the dictionary,and will list how many times equivalent values are added\n",
        "  list_to_remove = list(dictionary_lookup)    # change the 'dictionary_lookup' into a list, so that we can search for common words later.\n",
        "                                         \n",
        "\n",
        "  for singleitem in list_to_remove:                        #open the list\n",
        "    if singleitem.isalpha() == False:           \n",
        "      del dictionary_lookup[singleitem]                    #remove all the non-alpha numeric characters\n",
        "    elif len(singleitem) == 1:                  \n",
        "      del dictionary_lookup[singleitem]                    #remove any single character alpha-numeric characters \n",
        "  dictionary_lookup = dictionary_lookup.most_common(3000)  # only keep the most common 3000 words in the dictionary_lookup list  \n",
        "  return dictionary_lookup                                 # return the dictionary_lookup\n",
        "  \n",
        "  # In concusion, after this part of data collection and data cleaning, we can get a 3000 most common words from the train dataset.\n",
        "  # Spam emails have already becoming a huge problem on the internet, with Machine Learning, we can easily create a dictionary \n",
        "  # which contain most of the common words when we consider a spam email."
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_NG2-TpxQ1j"
      },
      "source": [
        "This function extracts feature columns and populates their values (Feature Matrix of 3000 comumns and rows equal to the number of email files). The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. Based on this the function also creates the Labelled Data Column. This function is used to extract the training dataset as well as the testing dataset and returns the Feature Dataset and the Label column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dmVW5xNlyOFc"
      },
      "source": [
        "# extract feature\n",
        "\n",
        "def extract_features(mail_dir):                               #Define a new function to extract the data\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]   \n",
        "                                                              # concatenates various path components with exactly one directory separator (‘/’), \n",
        "                                                              # and also open the list containing the names of the entries in the directory given by path\n",
        "  features_matrix = np.zeros((len(files),3000))               # Also use np.zero function to return a new array of 3000 columns and the same number of email files as rows with zeros.(template empty)\n",
        "  train_labels = np.zeros(len(files))                         # return another array contains the number of email files as row numbers.\n",
        "  count = 1;                                                  # the first count is 1\n",
        "  docID = 0;                                                  # start from index 0\n",
        "  for file in files:                                          # read each file from 'files' list\n",
        "    with open(file) as f:                                     # open each single file and name it as f\n",
        "      for itemcount, item in enumerate(f):                    # itemcount is the count of items, item is the item itself. Use enumerate method adds counter to an iterable and returns it, the returned object is a enumerate object\n",
        "        if itemcount == 2:                                    # if the count is 2\n",
        "          words = item.split()                                # we will split the line in to a list called words\n",
        "          for word in words:                                  # for each single item in list 'words'\n",
        "            wordID = 0                                        # like a start\n",
        "            for itemcount, item2 in enumerate(dictionary_lookup):   \n",
        "                                                              #we have to go open dictionary_lookup list. itemcount is the count of items, and item2 is the item in dictionary\n",
        "              if item2[0] == word:                            # if the first index/item in the list equal to word (in words)\n",
        "                wordID = itemcount                            # then we assign itemcount to wordID \n",
        "                features_matrix[docID,wordID] = words.count(word)    #count how many unique word in words   [1,1,2,3]   2：1， 1：2， 1：3\n",
        "      train_labels[docID] = 0;                                       # start\n",
        "      filepathTokens = file.split('/')                               # split each file\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]              # since python count from 0, we have to minus one as it is last word\n",
        "      if lastToken.startswith(\"spmsg\"):                       #according to the case instruction, when an title of an email begin with 'spmsg' is means file is of spam emails.\n",
        "                                                              # The function also analyzes the File Names of each email file and decides if it's a Spam or not based on the naming convention. \n",
        "                                                              # Based on this the function also creates the Labelled Data Column\n",
        "        train_labels[docID] = 1;                              #set train_label's first index as 1\n",
        "        count = count + 1                                     #the count will be added one if applicable\n",
        "      docID = docID + 1                                       #the count of docID will be added one if applicacble\n",
        "  return features_matrix, train_labels                        #turn the results\n",
        "\n",
        "  #In summary, after this part of coding, we will get a new functionThis function that can help us to extract the training dataset and the testing dataset \n",
        "  #in order to return the Feature Dataset and the Label column."
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbOV1Y4hxpiy"
      },
      "source": [
        "The section is the main Program that calls the above two functions and gets executed first. First it \"trains\" the model using model.fit function and Training Dataset. After that it scores the Test Data set by running the Trained Model with the Test Data set. At the end it prints the model performance in terms of accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "zoq-rE7Mx0pp"
      },
      "source": [
        "#import Train & Test data\n",
        "\n",
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML Algorithms/Naive_Bayes/train-mails'\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML Algorithms/Naive_Bayes/test-mails'"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "134lmhauyQxE",
        "outputId": "64a652c7-4f23-4a32-a646-ecbe66edc915"
      },
      "source": [
        "#Gaussian model is apply for the classification and it assumes that features follow a normal distribution.\n",
        "#call two functions\n",
        "\n",
        "dictionary_lookup = make_Dictionary(TRAIN_DIR)\n",
        "\n",
        "print (\"reading and processing emails from TRAIN and TEST folders\")\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)           \n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)  \n",
        "\n",
        "model = GaussianNB()\n",
        "\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\n",
        "model.fit(features_matrix, labels)                           #use train data to train the model\n",
        "print (\"Training completed\")\n",
        "print (\"testing trained model to predict Test Data labels\")\n",
        "predicted_labels = model.predict(test_features_matrix)       #use test data to perdict\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\n",
        "print (accuracy_score(test_labels, predicted_labels))        \n",
        "\n",
        "# Modle scores the test data by running the trained model with the test data.\n",
        "# Accurancy score is the percentage of correct perdiction, which is around 0.96 after we run the model."
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n",
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9653846153846154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_mPrvN586A"
      },
      "source": [
        "======================= END OF PROGRAM ========================="
      ]
    }
  ]
}